{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFyb0y7PUJOo"
   },
   "source": [
    "# Iberdrola Project - Phase 2 [Occupancy Detection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9vTr2NhcGAA"
   },
   "source": [
    "# Test GPU (Optional)\n",
    "Before Starting, kindly check the available GPU from the Google Server, GPU model and other related information. It might help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA enabled GPU Available? True\n",
      "GPU Number: 1\n",
      "Current GPU Index: 0\n",
      "GPU Type: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n",
      "GPU Capability: (8, 6)\n",
      "Is GPU Initialized yet? True\n",
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA enabled GPU Available?\", torch.cuda.is_available())\n",
    "print(\"GPU Number:\", torch.cuda.device_count())\n",
    "print(\"Current GPU Index:\", torch.cuda.current_device())\n",
    "print(\"GPU Type:\", torch.cuda.get_device_name(device=None))\n",
    "print(\"GPU Capability:\", torch.cuda.get_device_capability(device=None))\n",
    "print(\"Is GPU Initialized yet?\", torch.cuda.is_initialized())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 20:57:47.795647: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-29 20:57:47.838931: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-29 20:57:47.838967: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-29 20:57:47.840738: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-29 20:57:47.857446: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-29 20:57:48.707470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 20:57:49.533028: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-29 20:57:49.544549: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-29 20:57:49.544576: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgW7r0C9TuZk"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eMhBhz1CrMb3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import scipy\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import configparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import loadmat, savemat\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "03JA1kRfzoit"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['data_appliance', 'data_occupancy', 'house_number_array']>\n",
      "(38, 86400, 1)\n",
      "(38, 86400, 1)\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "fl_Data = h5py.File(os.path.join('Raw_Data/Phase_2_Exp_1/02_Dishwasher_Data_Matched_Occupancy.mat'), 'r')\n",
    "print(fl_Data.keys())\n",
    "X_Data = np.expand_dims(np.array(fl_Data['data_appliance']), axis=2)\n",
    "Y_Data = np.int_(np.expand_dims(np.array(fl_Data['data_occupancy']), axis=2))\n",
    "X_Data_shape = X_Data.shape\n",
    "Y_Data_shape = Y_Data.shape\n",
    "print(X_Data_shape)\n",
    "print(Y_Data_shape)\n",
    "sample_num = X_Data_shape[0]\n",
    "segment_length = X_Data_shape[1]\n",
    "num_channels = X_Data_shape[2]\n",
    "# Check for NaNs and InFs\n",
    "data = pd.Series(X_Data.ravel())\n",
    "print(data.isna().any())\n",
    "print(data.isin([np.inf, -np.inf]).any())\n",
    "data = pd.Series(Y_Data.ravel())\n",
    "print(data.isna().any())\n",
    "print(data.isin([np.inf, -np.inf]).any())\n",
    "del fl_Data\n",
    "sample_num = X_Data.shape[0]\n",
    "segment_length = X_Data.shape[1]\n",
    "num_channels = X_Data.shape[2]\n",
    "# Check for NaNs and InFs\n",
    "print(pd.Series(X_Data.ravel()).isna().any())\n",
    "print(pd.Series(X_Data.ravel()).isin([np.inf, -np.inf]).any())\n",
    "print(pd.Series(Y_Data.ravel()).isna().any())\n",
    "print(pd.Series(Y_Data.ravel()).isin([np.inf, -np.inf]).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### - Generating Load Event Pulses from Raw Device Data - #####\n",
      "(38, 86400, 1)\n",
      "[0 1]\n",
      "##### - Curating Pulses - #####\n",
      "(31, 86400, 1)\n",
      "(31, 86400, 1)\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Generate Load Event Pulses per channel\n",
    "print('##### - Generating Load Event Pulses from Raw Device Data - #####')\n",
    "Data_Pulses = np.zeros((sample_num,segment_length,X_Data.shape[2]))\n",
    "counter = 0\n",
    "thresh1 = 0\n",
    "for i in range(0,sample_num):\n",
    "    X_Data_Temp = X_Data[i,:,:]\n",
    "    for ii in range(0,X_Data.shape[2]):\n",
    "        X_Data_Temp_Ch = X_Data_Temp[:,ii]\n",
    "        for iii in range(0,segment_length):\n",
    "            X_Data_Temp_Ch_Point = X_Data_Temp_Ch[iii]\n",
    "            if X_Data_Temp_Ch_Point > thresh1:\n",
    "                X_Data_Temp_Ch_Point = 1\n",
    "            Data_Pulses[counter,iii,ii] = X_Data_Temp_Ch_Point\n",
    "    counter = counter + 1\n",
    "Data_Pulses = np.int_(Data_Pulses[0:counter,:,:])\n",
    "print(Data_Pulses.shape)\n",
    "print(np.unique(Data_Pulses))\n",
    "# Curate Data Pulses\n",
    "print('##### - Curating Pulses - #####')\n",
    "Data_Pulses_Curated = np.zeros((sample_num,segment_length,Data_Pulses.shape[2]))\n",
    "Data_Ocp_Curated = np.zeros((sample_num,segment_length,Y_Data.shape[2]))\n",
    "thresh2 = 1800\n",
    "thresh3 = 1800\n",
    "counter = 0\n",
    "for i in range(0,sample_num):\n",
    "    Data_Pulses_Temp = Data_Pulses[i,:,:]\n",
    "    Data_Ocp_Temp = Y_Data[i,:,:]\n",
    "    Data_Ocp_Temp_SUM = np.sum(Data_Ocp_Temp)\n",
    "    if (Data_Ocp_Temp_SUM <= np.round(0.01*segment_length)) or (Data_Ocp_Temp_SUM >= np.round(0.99*segment_length)):\n",
    "        continue\n",
    "    Data_Pulses_Curated[counter,:,:] = Data_Pulses_Temp\n",
    "    Data_Ocp_Curated[counter,:,:] = Data_Ocp_Temp\n",
    "    counter = counter + 1\n",
    "Data_Pulses_Curated = np.int_(Data_Pulses_Curated[0:counter,:,:])\n",
    "Data_Ocp_Curated = np.int_(Data_Ocp_Curated[0:counter,:,:])\n",
    "# Print\n",
    "print(Data_Pulses_Curated.shape)\n",
    "print(Data_Ocp_Curated.shape)\n",
    "print(np.unique(Data_Pulses_Curated))\n",
    "print(np.unique(Data_Ocp_Curated))\n",
    "# Save\n",
    "hfilew = h5py.File('Occupancy_Data_Curated.h5','w')\n",
    "hfilew.create_dataset('X_Data', data=Data_Pulses_Curated)\n",
    "hfilew.create_dataset('Y_Data', data=Data_Ocp_Curated)\n",
    "hfilew.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['X_Data', 'Y_Data']>\n",
      "(31, 86400, 1)\n",
      "(31, 86400, 1)\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "(31, 86400, 1)\n",
      "(31, 86400, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "fl_Data = h5py.File(os.path.join('Occupancy_Data_Curated.h5'), 'r')\n",
    "print(fl_Data.keys())\n",
    "# Extract\n",
    "X_Data = np.array(fl_Data['X_Data'])\n",
    "print(X_Data.shape)\n",
    "Y_Data = np.array(fl_Data['Y_Data'])\n",
    "print(Y_Data.shape)\n",
    "del fl_Data\n",
    "sample_num = X_Data.shape[0]\n",
    "segment_length = X_Data.shape[1]\n",
    "num_channels = X_Data.shape[2]\n",
    "# Check for NaNs and InFs\n",
    "print(pd.Series(X_Data.ravel()).isna().any())\n",
    "print(pd.Series(Y_Data.ravel()).isin([np.inf, -np.inf]).any())\n",
    "print(pd.Series(X_Data.ravel()).isna().any())\n",
    "print(pd.Series(Y_Data.ravel()).isin([np.inf, -np.inf]).any())\n",
    "# Shuffle\n",
    "ind_list = [i for i in range(sample_num)]\n",
    "shuffle(ind_list)\n",
    "X_Data_Shuffled = X_Data[ind_list,:,:]\n",
    "Y_Data_Shuffled = Y_Data[ind_list,:,:]\n",
    "print(X_Data_Shuffled.shape)\n",
    "print(Y_Data_Shuffled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Folds and Transform Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "segment_length = 3600  # 60 Minutes = 3600 Data Points (1 Hz sampling rate)\n",
    "offset_sec = 60  # 120 seconds offset for overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(A_Data, B_Data, segment_length, offset_amount, C1=True, C2=True):\n",
    "    A_Data_New = np.zeros((20000, A_Data.shape[2], segment_length))\n",
    "    B_Data_New = np.zeros((20000, B_Data.shape[2]), dtype=int)\n",
    "    # Reshape Array\n",
    "    A_Data_Reshaped = np.reshape(A_Data, (A_Data.shape[0]*A_Data.shape[1], A_Data.shape[2]))\n",
    "    B_Data_Reshaped = np.reshape(B_Data, (B_Data.shape[0]*B_Data.shape[1], B_Data.shape[2]))\n",
    "    ## Slide through and generate labels\n",
    "    counter = 0\n",
    "    for i in range(0,B_Data_Reshaped.shape[0]):\n",
    "        if (i*offset_amount+segment_length) >= A_Data_Reshaped.shape[0]:\n",
    "            continue\n",
    "        A_Data_Temp = np.transpose(A_Data_Reshaped[i*offset_amount:i*offset_amount+segment_length,:])\n",
    "        # A_Data_Temp_Mean = np.mean(A_Data_Temp)\n",
    "        A_Data_Temp_VAR = np.var(A_Data_Temp)\n",
    "        B_Data_Temp = B_Data_Reshaped[i*offset_amount:i*offset_amount+segment_length,:]\n",
    "        B_Data_Temp_Mean = np.mean(B_Data_Temp)\n",
    "        # B_Data_Temp_VAR = np.var(B_Data_Temp)\n",
    "        B_Data_Temp_SUM = np.sum(B_Data_Temp)\n",
    "        if C1 == True:\n",
    "            if A_Data_Temp_VAR <= 0:\n",
    "                continue\n",
    "        if C2 == True:\n",
    "            if (B_Data_Temp_SUM < np.round(0.01*segment_length)) or (B_Data_Temp_SUM > np.round(0.99*segment_length)):\n",
    "                continue\n",
    "        if B_Data_Temp_Mean > 0.5:\n",
    "            B_Label = 1\n",
    "        else:\n",
    "            B_Label = 0\n",
    "        if (pd.Series(A_Data_Temp.ravel()).isna().any() == False) and (pd.Series(A_Data_Temp.ravel()).isin([np.inf, -np.inf]).any() == False):\n",
    "            A_Data_New[counter,:,:] = A_Data_Temp\n",
    "            B_Data_New[counter,:] = B_Label\n",
    "            counter = counter + 1\n",
    "    A_Data_New = A_Data_New[0:counter,:,:]\n",
    "    B_Data_New = B_Data_New[0:counter,:]\n",
    "    return A_Data_New, B_Data_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 1\n",
    "sample_num = X_Data_Shuffled.shape[0]\n",
    "X_Train_F1 = X_Data_Shuffled[0:round((sample_num/5)*4),:,:]\n",
    "X_Test_F1 = X_Data_Shuffled[round((sample_num/5)*4):,:,:]\n",
    "Y_Train_F1 = Y_Data_Shuffled[0:round((sample_num/5)*4),:,:]\n",
    "Y_Test_F1 = Y_Data_Shuffled[round((sample_num/5)*4):,:,:]\n",
    "X_Train_F1_C, Y_Train_F1_C = transform_labels(X_Train_F1, Y_Train_F1, segment_length, offset_sec, True, True)\n",
    "X_Test_F1_C, Y_Test_F1_C = transform_labels(X_Test_F1, Y_Test_F1, segment_length, offset_sec, True, True)\n",
    "print('##### - Fold 1 - #####')\n",
    "print(X_Train_F1_C.shape)\n",
    "print(Y_Train_F1_C.shape)\n",
    "print(X_Test_F1_C.shape)\n",
    "print(Y_Test_F1_C.shape)\n",
    "labels, counts = np.unique(Y_Train_F1_C, return_counts=True)\n",
    "print(labels, counts)\n",
    "labels, counts = np.unique(Y_Test_F1_C, return_counts=True)\n",
    "print(labels, counts)\n",
    "del X_Train_F1, Y_Train_F1, X_Test_F1, Y_Test_F1\n",
    "hfilew = h5py.File('Occupancy_Data_Classification_Fold1.h5','w')\n",
    "hfilew.create_dataset('X_Train', data=X_Train_F1_C)\n",
    "hfilew.create_dataset('Y_Train', data=Y_Train_F1_C)\n",
    "hfilew.create_dataset('X_Test', data=X_Test_F1_C)\n",
    "hfilew.create_dataset('Y_Test', data=Y_Test_F1_C)\n",
    "hfilew.close()\n",
    "# Fold 2\n",
    "X_Train_F21 = X_Data_Shuffled[0:round((sample_num/5)*3),:,:]\n",
    "X_Train_F22 = X_Data_Shuffled[round((sample_num/5)*4):,:,:]\n",
    "X_Train_F2 = np.concatenate((X_Train_F21,X_Train_F22), axis=0)\n",
    "X_Test_F2 = X_Data_Shuffled[round((sample_num/5)*3):round((sample_num/5)*4),:,:]\n",
    "Y_Train_F21 = Y_Data_Shuffled[0:round((sample_num/5)*3),:,:]\n",
    "Y_Train_F22 = Y_Data_Shuffled[round((sample_num/5)*4):,:,:]\n",
    "Y_Train_F2 = np.concatenate((Y_Train_F21,Y_Train_F22), axis=0)\n",
    "Y_Test_F2 = Y_Data_Shuffled[round((sample_num/5)*3):round((sample_num/5)*4),:,:]\n",
    "X_Train_F2_C, Y_Train_F2_C = transform_labels(X_Train_F2, Y_Train_F2, segment_length, offset_sec, True, True)\n",
    "X_Test_F2_C, Y_Test_F2_C = transform_labels(X_Test_F2, Y_Test_F2, segment_length, offset_sec, True, True)\n",
    "print('##### - Fold 2 - #####')\n",
    "print(X_Train_F2_C.shape)\n",
    "print(Y_Train_F2_C.shape)\n",
    "print(X_Test_F2_C.shape)\n",
    "print(Y_Test_F2_C.shape)\n",
    "labels, counts = np.unique(Y_Train_F2_C, return_counts=True)\n",
    "print(labels, counts)\n",
    "labels, counts = np.unique(Y_Test_F2_C, return_counts=True)\n",
    "print(labels, counts)\n",
    "del X_Train_F2, Y_Train_F2, X_Test_F2, Y_Test_F2\n",
    "hfilew = h5py.File('Occupancy_Data_Classification_Fold2.h5','w')\n",
    "hfilew.create_dataset('X_Train', data=X_Train_F2_C)\n",
    "hfilew.create_dataset('Y_Train', data=Y_Train_F2_C)\n",
    "hfilew.create_dataset('X_Test', data=X_Test_F2_C)\n",
    "hfilew.create_dataset('Y_Test', data=Y_Test_F2_C)\n",
    "hfilew.close()\n",
    "# Fold 3\n",
    "X_Train_F31 = X_Data_Shuffled[0:round((sample_num/5)*2),:,:]\n",
    "X_Train_F32 = X_Data_Shuffled[round((sample_num/5)*3):,:,:]\n",
    "X_Train_F3 = np.concatenate((X_Train_F31,X_Train_F32), axis=0)\n",
    "X_Test_F3 = X_Data_Shuffled[round((sample_num/5)*2):round((sample_num/5)*3),:,:]\n",
    "Y_Train_F31 = Y_Data_Shuffled[0:round((sample_num/5)*2),:,:]\n",
    "Y_Train_F32 = Y_Data_Shuffled[round((sample_num/5)*3):,:,:]\n",
    "Y_Train_F3 = np.concatenate((Y_Train_F31,Y_Train_F32), axis=0)\n",
    "Y_Test_F3 = Y_Data_Shuffled[round((sample_num/5)*2):round((sample_num/5)*3),:,:]\n",
    "X_Train_F3_C, Y_Train_F3_C = transform_labels(X_Train_F3, Y_Train_F3, segment_length, offset_sec, True, True)\n",
    "X_Test_F3_C, Y_Test_F3_C = transform_labels(X_Test_F3, Y_Test_F3, segment_length, offset_sec, True, True)\n",
    "print('##### - Fold 3 - #####')\n",
    "print(X_Train_F3_C.shape)\n",
    "print(Y_Train_F3_C.shape)\n",
    "print(X_Test_F3_C.shape)\n",
    "print(Y_Test_F3_C.shape)\n",
    "labels, counts = np.unique(Y_Train_F3_C, return_counts=True)\n",
    "print(labels, counts)\n",
    "labels, counts = np.unique(Y_Test_F3_C, return_counts=True)\n",
    "print(labels, counts)\n",
    "del X_Train_F3, Y_Train_F3, X_Test_F3, Y_Test_F3\n",
    "hfilew = h5py.File('Occupancy_Data_Classification_Fold3.h5','w')\n",
    "hfilew.create_dataset('X_Train', data=X_Train_F3_C)\n",
    "hfilew.create_dataset('Y_Train', data=Y_Train_F3_C)\n",
    "hfilew.create_dataset('X_Test', data=X_Test_F3_C)\n",
    "hfilew.create_dataset('Y_Test', data=Y_Test_F3_C)\n",
    "hfilew.close()\n",
    "# Fold 4\n",
    "X_Train_F41 = X_Data_Shuffled[0:round((sample_num/5)*1),:,:]\n",
    "X_Train_F42 = X_Data_Shuffled[round((sample_num/5)*2):,:,:]\n",
    "X_Train_F4 = np.concatenate((X_Train_F41,X_Train_F42), axis=0)\n",
    "X_Test_F4 = X_Data_Shuffled[round((sample_num/5)*1):round((sample_num/5)*2),:,:]\n",
    "Y_Train_F41 = Y_Data_Shuffled[0:round((sample_num/5)*1),:,:]\n",
    "Y_Train_F42 = Y_Data_Shuffled[round((sample_num/5)*2):,:,:]\n",
    "Y_Train_F4 = np.concatenate((Y_Train_F41,Y_Train_F42), axis=0)\n",
    "Y_Test_F4 = Y_Data_Shuffled[round((sample_num/5)*1):round((sample_num/5)*2),:,:]\n",
    "X_Train_F4_C, Y_Train_F4_C = transform_labels(X_Train_F4, Y_Train_F4, segment_length, offset_sec, True, True)\n",
    "X_Test_F4_C, Y_Test_F4_C = transform_labels(X_Test_F4, Y_Test_F4, segment_length, offset_sec, True, True)\n",
    "print('##### - Fold 4 - #####')\n",
    "print(X_Train_F4_C.shape)\n",
    "print(Y_Train_F4_C.shape)\n",
    "print(X_Test_F4_C.shape)\n",
    "print(Y_Test_F4_C.shape)\n",
    "labels, counts = np.unique(Y_Train_F4_C, return_counts=True)\n",
    "print(labels, counts)\n",
    "labels, counts = np.unique(Y_Test_F4_C, return_counts=True)\n",
    "print(labels, counts)\n",
    "del X_Train_F4, Y_Train_F4, X_Test_F4, Y_Test_F4\n",
    "hfilew = h5py.File('Occupancy_Data_Classification_Fold4.h5','w')\n",
    "hfilew.create_dataset('X_Train', data=X_Train_F4_C)\n",
    "hfilew.create_dataset('Y_Train', data=Y_Train_F4_C)\n",
    "hfilew.create_dataset('X_Test', data=X_Test_F4_C)\n",
    "hfilew.create_dataset('Y_Test', data=Y_Test_F4_C)\n",
    "hfilew.close()\n",
    "# Fold 5\n",
    "sample_num = X_Data_Shuffled.shape[0]\n",
    "X_Train_F5 = X_Data_Shuffled[round((sample_num/5)*1):,:,:]\n",
    "X_Test_F5 = X_Data_Shuffled[0:round((sample_num/5)*1),:,:]\n",
    "Y_Train_F5 = Y_Data_Shuffled[round((sample_num/5)*1):,:,:]\n",
    "Y_Test_F5 = Y_Data_Shuffled[0:round((sample_num/5)*1),:,:]\n",
    "X_Train_F5_C, Y_Train_F5_C = transform_labels(X_Train_F5, Y_Train_F5, segment_length, offset_sec, True, True)\n",
    "X_Test_F5_C, Y_Test_F5_C = transform_labels(X_Test_F5, Y_Test_F5, segment_length, offset_sec, True, True)\n",
    "print('##### - Fold 5 - #####')\n",
    "print(X_Train_F5_C.shape)\n",
    "print(Y_Train_F5_C.shape)\n",
    "print(X_Test_F5_C.shape)\n",
    "print(Y_Test_F5_C.shape)\n",
    "labels, counts = np.unique(Y_Train_F5_C, return_counts=True)\n",
    "print(labels, counts)\n",
    "labels, counts = np.unique(Y_Test_F5_C, return_counts=True)\n",
    "print(labels, counts)\n",
    "del X_Train_F5, Y_Train_F5, X_Test_F5, Y_Test_F5\n",
    "hfilew = h5py.File('Occupancy_Data_Classification_Fold5.h5','w')\n",
    "hfilew.create_dataset('X_Train', data=X_Train_F5_C)\n",
    "hfilew.create_dataset('Y_Train', data=Y_Train_F5_C)\n",
    "hfilew.create_dataset('X_Test', data=X_Test_F5_C)\n",
    "hfilew.create_dataset('Y_Test', data=Y_Test_F5_C)\n",
    "hfilew.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Save for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['X_Test', 'X_Train', 'Y_Test', 'Y_Train']>\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "fl_Data = h5py.File(os.path.join(f'Occupancy_Data_Classification_Fold{fold_num}.h5'),'r')\n",
    "print(fl_Data.keys())\n",
    "X_Train = np.array(fl_Data['X_Train'])\n",
    "Y_Train = np.array(fl_Data['Y_Train'])\n",
    "X_Test = np.array(fl_Data['X_Test'])\n",
    "Y_Test = np.array(fl_Data['Y_Test'])\n",
    "train_data_dic = {\"X_Train\": X_Train,\n",
    "                  \"Y_Train\": Y_Train,\n",
    "                  \"X_Test\": X_Test,\n",
    "                  \"Y_Test\": Y_Test,\n",
    "                  \"X_Val\": X_Test,\n",
    "                  \"Y_Val\": Y_Test,\n",
    "                  }\n",
    "savemat(f\"Data/Data_Fold_{fold_num}.mat\", train_data_dic, format='5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qTwozk_BS94"
   },
   "source": [
    "Garbage Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1633010341331,
     "user": {
      "displayName": "Sakib Mahmud",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8lG2uTygQr7y6fmQUo67XXUtrCVGaEakj_P33Ft8=s64",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "ch2jmn3jKKiH",
    "outputId": "c9807e23-ff6c-428d-f6e3-4f5e6382e71d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc #Garbage Collector\n",
    "fl_Data = None\n",
    "X_Test = None\n",
    "X_Train = None\n",
    "X_Val = None\n",
    "Y_Test = None\n",
    "Y_Train = None\n",
    "Y_Val = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHq0FrX9iAsq"
   },
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yozhkF-OJWu2"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Mai6ZRMeiFvA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file 'Config_Train.ini' created\n"
     ]
    }
   ],
   "source": [
    "# CREATE OBJECT\n",
    "config_file = configparser.ConfigParser()\n",
    "# ADD NEW SECTION AND SETTINGS\n",
    "config_file[\"TRAIN\"] = {\n",
    "    'parentdir': '',  # Root or Parent Directory\n",
    "    'datafile': 'Data',  # Folder containing the dataset\n",
    "    'val_size': 0.0,  # Validation percentage for splitting\n",
    "    'q_order': 3,  # q-order for the Self-ONN or Super-ONN Models\n",
    "    'batch_size': 8,  # Batch Size, Change to fit hardware\n",
    "    'lossType': 'MSE',  # loss function: 'SoftM_CELoss' or 'SoftM_MSE' or 'MSE'\n",
    "    'optim_fc': 'SGD',  # 'Adam' or 'SGD'\n",
    "    'lr': 0.0001,  # learning rate\n",
    "    'stop_criteria': 'accuracy',  # Stopping criteria: 'loss' or 'accuracy'\n",
    "    'n_epochs': 500,  # number of training epochs\n",
    "    'epochs_patience': 10,\n",
    "    'lr_factor': 0.1,  # lr_factor, if val loss did not decrease for a number of epochs (epochs_patience) then decrease learning rate by a factor of lr_factor\n",
    "    'max_epochs_stop': 50,  # maximum number of epochs with no improvement in validation loss for early stopping\n",
    "    'num_folds': 1,  # number of cross validation folds\n",
    "    'load_model': False,  # load model: True or False\n",
    "    'load_model_path': f'Results_Classification\\ODNet_Occp_Classification\\Fold_{fold_num}\\ODNet_Occp_Classification_fold_{fold_num}.pt',  # specify path of pretrained model wieghts or set to False to train from scratch\n",
    "    'model_to_load': 'ODNet',  # choose one of the following models: 'CNN_1' 'CNN_2' 'CNN_2' 'CNN_3' 'SelfResNet18' 'ResNet'\n",
    "    'model_name': 'ODNet_Occp_Classification',  # choose a unique name for result folder\n",
    "    'aux_logits': False,  # Required for models with auxilliary outputs (e.g., InceptionV3)  \n",
    "    'fold_start': 1,  # The starting fold for training\n",
    "    'fold_last': 5,  # The last fold for training\n",
    "    'results_path': 'Results_Classification',  # main results folder\n",
    "}\n",
    "\n",
    "# SAVE CONFIG FILE\n",
    "with open(r\"Config_Train.ini\", 'w') as configfileObj:\n",
    "    config_file.write(configfileObj)\n",
    "    configfileObj.flush()\n",
    "    configfileObj.close()\n",
    "\n",
    "print(\"Config file 'Config_Train.ini' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWQQ-U2bJ2Js"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSLqe-8gJ2R8"
   },
   "outputs": [],
   "source": [
    "%run -i Train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file 'Config_Test.ini' created\n"
     ]
    }
   ],
   "source": [
    "# CREATE OBJECT\n",
    "config_file = configparser.ConfigParser()\n",
    "# ADD NEW SECTION AND SETTINGS\n",
    "config_file[\"TEST\"] = {\n",
    "    'parentdir': '',  # Root or Parent Directory\n",
    "    'datafile': 'Data',  # Folder containing the dataset\n",
    "    'batch_size': 8,  # Batch Size, Change to fit hardware\n",
    "    'lossType': 'MSE',  # loss function: 'SoftM_CELoss' or 'SoftM_MSE' or 'MSE'\n",
    "    'num_folds': 5,  # number of cross validation folds\n",
    "    'CI': 0.9,  # Confidence interval (missied cases with probability>=CI will be reported in excel file)\n",
    "    'load_model': False,  # specify path of pretrained model wieghts or set to False to train from scratch\n",
    "    'load_model_path': f'Results_Classification\\ODNet_Occp_Classification\\Fold_1\\ODNet_Occp_Classification_fold_1.pt',  # specify path of pretrained model wieghts or set to False to train from scratch\n",
    "    'labeled_data': True,  # set to true if you have the labeled test set\n",
    "    'model_name': 'ODNet_Occp_Classification',  # name of the saved model\n",
    "    'aux_logits': False,  # Required for models with auxilliary outputs (e.g., InceptionV3)  \n",
    "    'fold_start': 1,  # The starting fold for training\n",
    "    'fold_last': 5,  # The last fold for training\n",
    "    'N_steps': 1000,  # The last fold for training\n",
    "    'results_path': 'Results_Classification',  # main results folder\n",
    "}\n",
    "# SAVE CONFIG FILE\n",
    "with open(r\"Config_Test.ini\", 'w') as configfileObj:\n",
    "    config_file.write(configfileObj)\n",
    "    configfileObj.flush()\n",
    "    configfileObj.close()\n",
    "print(\"Config file 'Config_Test.ini' created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i Test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tgW7r0C9TuZk",
    "Kir80l1FKPXB",
    "vMHdM26iekBm",
    "VUciLEJeyvyN",
    "WesN7p7AeYK4"
   ],
   "machine_shape": "hm",
   "name": "1D_CNN_Segmentation_End2End_Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wsl_miniconda_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
